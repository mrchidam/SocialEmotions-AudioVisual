ANALYZING SOCIAL EMOTIONS IN AUDIO AND VIDEO CONTENT
Abstract
The rapid development of the World Wide Web has resulted in a large database of online videos containing product, service or movie reviews. Analysis of these videos manually for classification and sentiment prediction is a tedious process. The proposed work focuses on automatic analysis of the speaker’s sentiment in online videos containing movie reviews and classifying it either as positive or negative. This approach focuses on extracting acoustic features by applying a set of acoustic low-level descriptors, visual features by computing facial expressions in every frame and linguistic features by applying speech to text conversion. The overall sentiment of the video is predicted by applying a weightage based fusion algorithm on the classification of all the three features. This approach achieves an average accuracy of % for movie reviews on ICT-MMMO Database which contains 370 movie review videos. This indicates that sentiment prediction based on acoustic and visual features in addition to linguistic features will result in higher accuracy when compared to standalone linguistic based prediction.
Keywords: sentiment analysis, binary valence, multimodal emotion recognition
INTRODUCTION
Automatic analysis of written reviews in terms of binary valence estimation was considerably studied in the last decade. Previous studies1, 2 classify product and services reviews and report robust results for the application domain. Peter Turney’s work1 produces accuracy of 84 percent for automobile reviews. On the other hand, written movie reviews appear to be rather difficult: in Turney’s work1, 66 percent accuracy was estimated for binary valence estimation of written movie reviews with the same method. One main challenge in the classification of textual movie reviews is that affective words often do not relate to the reviewer’s opinion, rather it relates to the elements of the movie. For example, affective words that we associate with strongly negative valence, such as “terrifying” can be used in a positive valence review of a horror movie.
Bjorn Schuller’s work9 proposed the use of higher-level knowledge from various online sources to better model the semantic relationships between the words in written movie reviews. A large database, called the Metacritic database (www.metacritic.com) which consists of more than lakh instances of written movie reviews was used for a robust data-oriented approach to classify written movie review. Contextual knowledge can be absorbed by depending on n-gram features. The estimation of n-gram features usually requires large amounts of data for training.
Against linguistics, vocal expressions – like prosody and laughter – and facial expressions shall be taken for consideration for a holistic analysis of the speaker’s sentiment. By combining audio and video features like the ones often used in recognition of emotion10 with text-based sentiment classification. Therefore, improving Schuller’s work9, we introduce classification of movie reviews using multimodal sentiment analysis, which can be easily applied in multimedia retrieval and labeling of infinite number of online videos.
RELATED WORKS
The proposed work is closely related to two research fields: text-based sentiment analysis, which has been studied widely in the last decade in computational linguistics and audio-visual emotion recognition that comes under speech processing and computer vision.
Text-based sentiment analysis is a growing area of work where it deals about automatically identifying the sentiment of a textual document such as movie reviews1, 2, product or service reviews, news articles, or blogs. There are two main problems associated with this area: cross-domain3 and cross-language portability. There is no much work regarding the extension of sentiment analysis to other modalities such as speech or facial expressions except two. First, in Stephan Raaijmakers and his colleagues’ work5, subjectivity identification is done by analyzing speech and text together. However, this work did not speak about sentiment analysis. Second, in a pre-study on 47 English review videos6, it has been proved that audio and video features can serve as an alternative to textual features for sentiment analysis.
Zhihong Zeng and his colleagues provide a recent survey of dimensional and categorical emotion recognition7. In the field of video retrieval, a new area of research which suggests the multimodal fusion of language, acoustic features, and visual gestures, such as the Video Information Retrieval Using Subtitles (Virus) project which uses all the three modalities to perform video retrieval8.
Despite various above mentioned publications that deals with text-based sentiment analysis and multimodal emotion recognition, an extensive study comparing in-domain, cross-domain and open-domain sentiment analysis from acoustic, visual and linguistic features extracted from online videos containing reviews doesn’t exists, to the best of our knowledge. Thus, the proposed work can be seen as a first attempt to evaluate these different features of sentiment analysis and to create an impact of the accuracies for classification of a fresh database of online videos containing spoken movie reviews.
PROPOSED WORK
The Proposed system extracts acoustic, video and linguistic features to aid in sentiment analysis.
ACOUSTIC FEATURES
The proposed system extracts acoustic features by applying a large set of acoustic low-level descriptors (LLD) along with its derivatives integrated with suitable statistical functionals to record speech dynamics within a turn. A turn is a pronouncement between speech pauses. The features and functionals are determined using a toolkit which analyzes audio. It is called openSMILE6. The total number of features in the audio feature set is 1,941. It is identical to a feature set employed elsewhere4. It is composed of 6 voicing-related LLD × 32 functionals, 6 delta coefficients of the voicing-related LLD × 19 functionals, 25 energy/spectral-related LLD × 42 functionals, 25 delta coefficients of the energy/spectral LLD × 23 functionals, and 10 voiced/unvoiced durational features.
In order to compact the total size of the resulting feature space, the system applies a cyclic correlation-based feature subset selection (CFS) 7 using the training set of each of the three-fold cross-validation experiments (detailed later). This results in a selection of 78, 74, and 71 acoustic features for the three folds.
VISUAL FEATURES
Since, only one person is present in each video clip usually facing the camera, the visual features are automatically extracted from video sequences. Current technology for facial tracking8 is efficient with respect to our dataset.
Before we compute facial features and deduce a set of basic facial expressions and the direction of eye gaze using the commercial software Okao Vision, we detect the face in every frame. The most important facial expression that we use is smile. We use smile intensity from 0 to 100, which is returned by the software. In addition to smile intensity, we also apply gaze direction in the form of horizontal and vertical angles in degrees.
In order to complement these features, we processed all review videos using a 3D head-pose tracker based on the Generalized Adaptive View-based Appearance Model9. This method improves the tracker robustness and precision by automatically acquiring key frames representing the head at different orientations. The head’s 3D position and orientation is estimated by the tracker at each frame. This data can be used to recognize absolute poses. Poses refer to head tilt or head down. In addition to absolute poses, head gestures such as head nods and shakes are also recognized from that data. Both sets of features were computed at the same rate as the original videos: 30 Hz.
Similar to audio feature-extraction method, we computed statistical functionals from the raw video feature vector sequences to generate a fixed number of video descriptors for each turn. We computed the mean and standard deviation over a complete spoken utterance for every video feature stream. This resulted in a video feature vector size to be 2 × 10 = 20 features. This size was then compacted via CFS to a set of 6 features, on an average.
LINGUISTIC FEATURES
The linguistic features can be analyzed using a simple unsupervised learning algorithm to classify them as positive or negative. The classification method is based on Peter D. Turney’s work. The semantic orientation of a phrase is calculated as the mutual information between the given phrase and the word “excellent” minus the mutual information between the given phrase and the word “poor”. The PWMI-IR algorithm is employed to estimate the semantic orientation of a phrase (Turney 2001). PWMI-IR uses Point wise Mutual Information (PWMI) and Information Retrieval (IR) to measure the similarity of pairs of words or phrases.

The first step is to use a part-of-speech tagger to identify phrases in the input text that contain adjectives or adverbs (Brill,1994).The phrase which is extracted is used to compute the semantic orientation which can be calculated as follows
PMI(word1,word2)=log_2⁡((p(word1 & word2)/(p(word1)  p(word2) )))

Here, p (word1 & word2) is the probability that word1 and word2 co-occur. If the words are statistically independent, then the probability that they co-occur is given by the product p(word1) p(word2).

After computing the PWMI, the semantic orientation (SO) of the phrase can be calculated using the following formula
SO(phrase)=PMI(phrase,"excellent")-PMI(phrase,"poor")

SO is positive when phrase is more strongly associated with “excellent” and negative when phrase is more strongly associated with “poor”.
PWMI is computed by giving a search query to the AltaVista advanced database to measure the number of hits with the database. The above two equations can be expressed in log-odd ratios (Agresti, 1996)
SO(phrase)=log_2⁡[(hits(phrase NEAR "excellent")  hits("poor"))/(hits(phrase NEAR "poor")  hits("excellent") )]

The third step is to calculate the average semantic orientation of the phrases in the given review and classify the review as recommended if the average is positive and otherwise not recommended.

CLASSIFICATION AND FUSION
